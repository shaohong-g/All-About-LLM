{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "  api_version=os.getenv(\"AZURE_OPENAI_VERSION\")\n",
    ")\n",
    "model = os.getenv(\"AZURE_OPENAI_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The use of the Oxford comma (also known as the serial comma) is a matter of style, and whether or not it should always be used depends on the style guide or personal preference you follow. An Oxford comma is the final comma in a list of three or more items, placed before the word \"and\" or \"or.\" For example:\\n\\n- With Oxford comma: I love my parents, Taylor Swift, and pizza.\\n- Without Oxford comma: I love my parents, Taylor Swift and pizza.\\n\\nThe Oxford comma is often recommended because it can reduce ambiguity. For instance:\\n\\n- Without Oxford comma: I’d like to thank my parents, Oprah Winfrey and God. (This implies the parents are Oprah Winfrey and God.)\\n- With Oxford comma: I’d like to thank my parents, Oprah Winfrey, and God. (Clearly separates into three distinct entities.)\\n\\n### When to Use the Oxford Comma\\n1. **Clarity**: Use an Oxford comma if omitting it would create confusion or ambiguity.\\n2. **Style Guides**: Certain style guides, such as **The Chicago Manual of Style** and **APA Style**, recommend always using the Oxford comma. Others, like the **Associated Press (AP) Stylebook**, advise omitting it unless clarity demands it.\\n3. **Consistency**: If you\\'re writing for yourself or in a context without strict style guidelines, it’s a good idea to pick one approach (using or omitting the Oxford comma) and apply it consistently.\\n\\n### Bottom Line\\nYou don’t *have* to always use the Oxford comma, but it’s often helpful and commonly used in formal and academic writing. Many writers and editors advocate for always including it to avoid potential confusion.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BASIC\n",
    "text_prompt = \"Should oxford commas always be used?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "                {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In large language models (LLMs), the terms \"system,\" \"user,\" and \"assistant\" refer to different roles within a conversation. \"System\" provides instructions or sets the context for the LLM, \"user\" represents the user's input, and \"assistant\" is the LLM's response. \n",
    "\n",
    "**Elaboration:**\n",
    "- System: This role is used to provide instructions, guidelines, or context for the LLM's behavior. It's often used to define the LLM's personality, style, or desired output format.\n",
    "- User: This role represents the user's input, which can be a question, command, or any other kind of prompt. It's the starting point for the LLM to generate a response.\n",
    "- Assistant: This role represents the LLM's response to the user's input. It's the output generated by the model based on the instructions and user input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All parameters explained:\n",
    "1. **temperature**: Controls the randomness of the output. A value of 0 makes the output more deterministic. (Higher values increase creativity)\n",
    "2. **max_tokens**: The maximum number of tokens (length) in the generated output.\n",
    "3. **top_p**: Controls diversity via nucleus sampling. (Limits token selection to the most probable options, whose cumulative probability meets a specified threshold, balancing diversity and coherence.) Lower values result in high-probability tokens, leading to more predictable and focused outputs. This is beneficial for tasks requiring precision and factual accuracy.\n",
    "4. **frequency_penalty**: Reduces the likelihood of repeating words or phrases by penalizing frequently used tokens.\n",
    "5. **presence_penalty**:  Promotes novelty by penalizing tokens that have already appeared.\n",
    "6. **stop**: Specifies stopping sequences to end the generated text appropriately.\n",
    "\n",
    "Considerations\n",
    "- temperature and top_p are functionally similar, hence it is recommended to tune either one of them and NOT both.\n",
    "- max_tokens include both the input tokens and the output tokens. Use this to generate a shorter response or lower api cost. (< ~200 token)\n",
    "- frequency_penalty had a range of -2.0 and 2.0. Negative values encourage the reuse of tokens, increasing repetition.\n",
    "- presence_penalty had a range of -2.0 and 2.0. It penalizes any token that has already been used in the output (even if it appeared only once)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three basic guidelines to creating prompts:\n",
    "\n",
    "Show and tell. Make it clear what you want either through instructions, examples, or a combination of the two. If you want the model to rank a list of items in alphabetical order or to classify a paragraph by sentiment, show it that's what you want.\n",
    "\n",
    "Provide quality data. If you're trying to build a classifier or get the model to follow a pattern, make sure that there are enough examples. Be sure to proofread your examples — the model is usually smart enough to see through basic spelling mistakes and give you a response, but it also might assume this is intentional and it can affect the response.\n",
    "\n",
    "Check your settings. The temperature and top_p settings control how deterministic the model is in generating a response. If you're asking it for a response where there's only one right answer, then you'd want to set these lower. If you're looking for more diverse responses, then you might want to set them higher. The number one mistake people use with these settings is assuming that they're \"cleverness\" or \"creativity\" controls.\n",
    "\n",
    "- add `Tl;dr` to summarize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Postgres SQL tables, with their properties:\n",
      "#\n",
      "# Employee(id, name, department_id)\n",
      "# Department(id, name, address)\n",
      "# Salary_Payments(id, employee_id, amount, date)\n",
      "#\n",
      "### A query to list the names of the departments which employed more than 10 employees in the last 3 months\n",
      "\n",
      " query: \n",
      "To list the names of the departments that employed more than 10 employees in the last 3 months, you can use the following SQL query:\n",
      "\n",
      "```sql\n",
      "SELECT d.name AS department_name\n",
      "FROM Department d\n",
      "JOIN Employee e ON d.id = e.department_id\n",
      "WHERE e.id IN (\n",
      "    SELECT DISTINCT employee_id\n",
      "    FROM Salary_Payments\n",
      "    WHERE date >= CURRENT_DATE - INTERVAL '3 months'\n",
      ")\n",
      "GROUP BY d.id, d.name\n",
      "HAVING COUNT(e.id) > 10\n"
     ]
    }
   ],
   "source": [
    "prompt = \"### Postgres SQL tables, with their properties:\\n#\\n# Employee(id, name, department_id)\\n# Department(id, name, address)\\n# Salary_Payments(id, employee_id, amount, date)\\n#\\n### A query to list the names of the departments which employed more than 10 employees in the last 3 months\\n\\n query: \"\n",
    "print(prompt)\n",
    "response = client.chat.completions.create(\n",
    "  model = model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\": prompt}],\n",
    "  temperature=0,\n",
    "  max_tokens=150,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0,\n",
    "  stop=[\"#\",\";\"])\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionUsage(completion_tokens=103, prompt_tokens=87, total_tokens=190, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See token usage\n",
    "response.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/function-calling?tabs=non-streaming%2Cpython"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
