{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "  api_version=os.getenv(\"AZURE_OPENAI_VERSION\")\n",
    ")\n",
    "model = os.getenv(\"AZURE_OPENAI_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The use of the Oxford comma—also known as the serial comma—is a matter of style rather than an absolute rule, and opinions vary depending on the context, audience, and style guide being followed. Here\\'s some information to help you decide whether to use it:\\n\\n### What is the Oxford comma?\\nThe Oxford comma is the comma placed before the conjunction (usually \"and\" or \"or\") in a list of three or more items. For example:\\n- With Oxford comma: \"I bought apples, oranges, and bananas.\"\\n- Without Oxford comma: \"I bought apples, oranges and bananas.\"\\n\\n### Arguments for Using the Oxford Comma:\\n1. **Clarity**: It can help avoid ambiguity. For example:\\n   - Without Oxford comma: \"I’d like to thank my parents, Oprah Winfrey and God.\"\\n     (This could be misinterpreted as suggesting your parents are Oprah and God!)\\n   - With Oxford comma: \"I’d like to thank my parents, Oprah Winfrey, and God.\"\\n     (This makes it clear the parents, Oprah, and God are distinct entities.)\\n2. **Consistency**: Using the Oxford comma consistently avoids any confusion and aligns with certain formal style guides.\\n\\n### Arguments Against Using the Oxford Comma:\\n1. **Simplicity**: If there’s no risk of ambiguity, some argue the Oxford comma is unnecessary and takes up extra space.\\n2. **Style Preferences**: Some style guides—such as the *Associated Press (AP)* Stylebook—omit the Oxford comma in most cases, except when its absence would cause confusion.\\n\\n### Style Guides and Professional Contexts:\\n- **Use the Oxford comma**: Many academic, literary, and formal style guides recommend the Oxford comma, including *The Chicago Manual of Style* and *The MLA Handbook.*\\n- **Omit the Oxford comma**: Some journalism and news-oriented publications, following the AP Stylebook, prefer to omit the Oxford comma unless it’s needed for clarity.\\n\\n### When in doubt:\\n- Use the Oxford comma if you’re writing in a formal or academic context, or if clarity is a concern.\\n- Check the style guide or preferences for your specific field, workplace, or audience.\\n\\nUltimately, whether to use the Oxford comma is a matter of personal or institutional choice, but clarity and consistency should always guide your decision.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BASIC\n",
    "text_prompt = \"Should oxford commas always be used?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "                {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In large language models (LLMs), the terms \"system,\" \"user,\" and \"assistant\" refer to different roles within a conversation. \"System\" provides instructions or sets the context for the LLM, \"user\" represents the user's input, and \"assistant\" is the LLM's response. \n",
    "\n",
    "**Elaboration:**\n",
    "- System: This role is used to provide instructions, guidelines, or context for the LLM's behavior. It's often used to define the LLM's personality, style, or desired output format.\n",
    "- User: This role represents the user's input, which can be a question, command, or any other kind of prompt. It's the starting point for the LLM to generate a response.\n",
    "- Assistant: This role represents the LLM's response to the user's input. It's the output generated by the model based on the instructions and user input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All parameters explained:\n",
    "1. **temperature**: Controls the randomness of the output. A value of 0 makes the output more deterministic. (Higher values increase creativity)\n",
    "2. **max_tokens**: The maximum number of tokens (length) in the generated output.\n",
    "3. **top_p**: Controls diversity via nucleus sampling. (Limits token selection to the most probable options, whose cumulative probability meets a specified threshold, balancing diversity and coherence.) Lower values result in high-probability tokens, leading to more predictable and focused outputs. This is beneficial for tasks requiring precision and factual accuracy.\n",
    "4. **frequency_penalty**: Reduces the likelihood of repeating words or phrases by penalizing frequently used tokens.\n",
    "5. **presence_penalty**:  Promotes novelty by penalizing tokens that have already appeared.\n",
    "6. **stop**: Specifies stopping sequences to end the generated text appropriately.\n",
    "\n",
    "Considerations\n",
    "- temperature and top_p are functionally similar, hence it is recommended to tune either one of them and NOT both.\n",
    "- max_tokens include both the input tokens and the output tokens. Use this to generate a shorter response or lower api cost. (< ~200 token)\n",
    "- frequency_penalty had a range of -2.0 and 2.0. Negative values encourage the reuse of tokens, increasing repetition.\n",
    "- presence_penalty had a range of -2.0 and 2.0. It penalizes any token that has already been used in the output (even if it appeared only once)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three basic guidelines to creating prompts:\n",
    "\n",
    "Show and tell. Make it clear what you want either through instructions, examples, or a combination of the two. If you want the model to rank a list of items in alphabetical order or to classify a paragraph by sentiment, show it that's what you want.\n",
    "\n",
    "Provide quality data. If you're trying to build a classifier or get the model to follow a pattern, make sure that there are enough examples. Be sure to proofread your examples — the model is usually smart enough to see through basic spelling mistakes and give you a response, but it also might assume this is intentional and it can affect the response.\n",
    "\n",
    "Check your settings. The temperature and top_p settings control how deterministic the model is in generating a response. If you're asking it for a response where there's only one right answer, then you'd want to set these lower. If you're looking for more diverse responses, then you might want to set them higher. The number one mistake people use with these settings is assuming that they're \"cleverness\" or \"creativity\" controls.\n",
    "\n",
    "- add `Tl;dr` to summarize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Postgres SQL tables, with their properties:\n",
      "#\n",
      "# Employee(id, name, department_id)\n",
      "# Department(id, name, address)\n",
      "# Salary_Payments(id, employee_id, amount, date)\n",
      "#\n",
      "### A query to list the names of the departments which employed more than 10 employees in the last 3 months\n",
      "\n",
      " query: \n",
      "To list the names of the departments that employed more than 10 employees in the last 3 months, you can use the following SQL query:\n",
      "\n",
      "```sql\n",
      "SELECT d.name AS department_name\n",
      "FROM Department d\n",
      "JOIN Employee e ON d.id = e.department_id\n",
      "WHERE e.id IN (\n",
      "    SELECT DISTINCT employee_id\n",
      "    FROM Salary_Payments\n",
      "    WHERE date >= CURRENT_DATE - INTERVAL '3 months'\n",
      ")\n",
      "GROUP BY d.id, d.name\n",
      "HAVING COUNT(e.id) > 10\n"
     ]
    }
   ],
   "source": [
    "prompt = \"### Postgres SQL tables, with their properties:\\n#\\n# Employee(id, name, department_id)\\n# Department(id, name, address)\\n# Salary_Payments(id, employee_id, amount, date)\\n#\\n### A query to list the names of the departments which employed more than 10 employees in the last 3 months\\n\\n query: \"\n",
    "print(prompt)\n",
    "response = client.chat.completions.create(\n",
    "  model = model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\": prompt}],\n",
    "  temperature=0,\n",
    "  max_tokens=150,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0,\n",
    "  stop=[\"#\",\";\"])\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionUsage(completion_tokens=103, prompt_tokens=87, total_tokens=190, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See token usage\n",
    "response.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/function-calling?tabs=non-streaming%2Cpython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
